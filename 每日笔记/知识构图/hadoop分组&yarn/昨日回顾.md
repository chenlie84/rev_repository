1.mapreduce分区和reducetask数量

​	默认：Hash分区   HashPartitioner

​	getpartition（key2，value2，numreducetask）{

​	(key2.hashcode & Integer.maxvalue) %numreducetask

}

​	extends  Partitioner{

​	getpartition（）{

​	}

}

​	reducetask数量：默认只有1个reducetask

​	reducetask>= 分区的数量 或者是1

2.排序和序列化

​	hadoop中：

​	1）Writable：只完成序列化

​	2）WritableCompareable：序列化和排序

​			compareTo完成字段间的排序

​	默认按照key2完成字典排序

3.计数器

​	context.getConuter("","")

​	context.getConuter(enum)

​	counter.increment(1L)

4.combiner规约

​	又称为map端的reduce

​	extends  Reducer{

}

5.mapTask和reduceTask工作机制

​	block-->split-->maptask-->kvbuffer-->分区-->排序-->规约-->本地磁盘-->merge-->fecth-->merge(归并排序)-->分组-->reducetask-->hdfs

6.数据压缩

​	snappy压缩

7.join

​	1）reduce：相同的key，value聚到一起进行关联

​	2）map join：小文件进行块缓存

​				setup（）加载块缓存

​				map端读取另一个文件数据，进行关联