---
typora-root-url: E:\每日笔记\img
---

# Hadoop03 

### 1. hdfs

1. **详细介绍**

   1. **hdfs** : 分布式文件系统 , 用于数据的存储 **图**   3个节点 每个节点 大小 40g

      1. 问题1 ?: 当Hdfs中储存的数据 , 存满的时候

         1. 横向扩展节点
         2. 纵向扩展 ; 在一台机子上 始终会达到上限

      2. 问题2 ? : 上传的文件大小为50G , 怎么办

         1. 将50G大小文件进行分块 , 默认128M

      3. 问题 : hdfs 如何解决数据丢失

         1. 副本机制 : 默认3个

      4. 问题 : 基于上面的问题 , 一个文件是如何存储的?

         1. 进行分块存储,有其自身的随机储存,由namenode维护元数据的信息, 记录某个块在那个节点上,不会记录具体的位置信息,这个由datanode记录

            ![1562118653012](/C:/Users/CHEER-~1/AppData/Local/Temp/1562118653012.png)

            最后一个块占用磁盘空间为实际数据的大小即 44m

         2. 分块之后的数据,通过元数据进行关联

   2. hdfs只是Hadoop提供的文件系统之一

      1. localfs:
      2. webhdfs : hue中整合hdfs

   3. **hdfs的设计目标**

      1. 硬件错误是常态 Hadoop往往部署在廉价服务器上
      2. 数据流的访问 : 流式的访问数据 , 离线数据处理  , 常用于批量处理数据 
      3. 大数据 适合处理 G T P 级别数据 , 处理小数据 , 会造成namenode 管理元数据压力增大
      4. 简单的相关模型 ,  一次写入 多次修改
      5. 移动计算比移动数据便宜
      6. 多种软硬件的可移植性

   4. hdfs的来源 GFS 

2. **基础架构**

   1.一个主节点nn 会跟着从节点dn

   关系

   1. 心跳机制(heartbeat)
      1. 每6秒会向主节点 汇报 从节点的生死状态, 当一个dn死掉 , nn会从其他dn 中重新复制一个副本,保证副本数为3, (balanceing: 完成数据的平衡)
   2. client : 用于文件的上传与下载 和删除等操作
   3. 2nn  : 合并元数据
   4. datanode : 管理文件数据 , 最终落地点 是磁盘
      1. 维护了blockid 和 datanode本地文件之间的关系
   5. namenode: 管理元数据  元数据默认是在本地磁盘 , 但启动服务时候会加载到内存. 停止的时候又会回到磁盘
      1. 保存文件 , block , datanode之间的映射关系.****

3. **hdfs的副本机制与块存储**

   1. 副本的默认放置策略:
      1. 第一个副本默认放置在上传文件的节点上
      2. 第二个副本放置在和第一副本相同机架上的不同节点
      3. 第三个放置在不同于1.2副本机架的随机机架上的随机节点
   2. 块存储
      1. 2.X      128m
      2. 1.x       64m
      3. 优点
   3. 块缓存 读取hdfs中的文件的时候,将比较小的数据 , 且经常读取 , 此时可以将块进行缓存
      1. join ,将两个文件进行链接处理 , 如果文件比较小的情况下 ,会进行缓存
      2. distributedCache用于对块进行缓存
   4. hdfs**权限验证**
      1. dfs.permissions.enabled默认是true开启的状态,只会根据用户和组进行权限的认证 , 权限认证比较薄弱 , 能过伪造root用户

### 2.**元数据的管理**

 1. 元数据 : 默认情况下 , 一条大概 150B

     	1. fsimage : 最终的元数据 会保存到fisimage中
     	2. edits: 当进行文件操作的时候,首先会将操作记录保存到edits中, 而不是fsimage
     	3. dfs.namenode.name.dir 指定fsimage的存储位置
     	4. dfs.namenode.edits.dir 配置的edits的存储位置

 2. fsimage文件中文件信息的查看

     	1. 使用命令 hdfs oiv .......... 

 3. edtis文件查看

     	1. hdfs oev ...........

 4. namenode如何管理员数据, snn如何辅助管理fsImage 与edtis文件 , 负责元数据的合并

     	1. 合并的条件 每个隔一个小时, 进行合并
         1. 2nn 通知 nn 要进行元数据的合并 , 建立检查点(CheckPoint)
         2. 为了不让edits不停止服务 ,生成了一个edits.new文件 , 用于接着写入新的操作记录
         3. NN 将 fismage 和 edits 通过Http 协议发送给2NN 的 fsimage 和 edits  , 进行合并
         4. 2NN 进行fismage 和 Edits 的元数据合并 成一个 fsimage.ckpt (ckpt 检查点)
         5. 2NN将合并后的fsimage 发送给nn, 并且改名为fsimage 
         6. 将edits.new改名为 edits
         7. nn 就有了新的fsimage 和 edits 并且将原来的旧的文件进行替换
         8. 服务器启动之初,会做一次检查点,进行元数据合并
         9. 2.X 默认是一个小时
         10. edits 100万条之后 , 就会合并edits 

    ![1562126047631](/C:/Users/CHEER-~1/AppData/Local/Temp/1562126047631.png)


​    

### 3.**读写流程** (********************重要***)

1. 写入过程
   1. client 客户端向namenode 提交上传的请求, 验证client的身份, 
   2. 有权限nn就会返回响应, 
   3. client可以提交文件(才是真正的提交),验证上传文件是否存在,并且请求上传位置
   4. NN返回结果 , 告知client可以上传 ,并且返回上传列表,(将大于128m数据进行分块,按照块的顺序写进节点中, 一个节点储存列表)
   5. client会跟datanode建立链接 , 以流的方式写到 dn中 , 写入过程中 按照packet 包的方式 ,默认一个包64K 128M 分成若干个64K的包进行写入
   6. 建立pipeline管道 , 将数据以块的顺序依次写入2 ,3 节点 
   7. 会依次的返回Ack确认队列,告知存储已经完成, 
      1. 在校验文件的时候 , 通过checksum
   8. client接受响应 ,文件储存成功,关闭流.
   9. client告知NameNode文件存储成功. 
2. 读取过程
   1. client请求读取文件地址 
   2. NN响应client读取地址 (心跳机制 返回节点信息 , 返回三个块其中一个就行)
   3. client建立输入流, 用于读取数据 , **并行读取**,通过元数据,识别块的顺序,然后进行合并,每一个块完成都要进行验证, 进行checkSum验证
      1. 为什么链接Node01, ??
         1. **短路读取**/**就近读取** , 通过网络拓扑 ,找到最近的距离,进行读取 , 机架感知
   4. 读取完成后 , 关闭输入流



### 4.**hdfs的api的操作**** (********************重要***)

1. 仓库的下载 , 到CDH给的仓库中获取

2. 解决winUtils的问题

3. 使用文件系统方式访问hdfs数据(**掌握**)

   1. Configuration:设置读取配置文件的类

   2. FileSystem : 文件系统类 . 通过fs.defaultFS   例如hdfs:// , 判断访问的是哪个系统. 

   3. 获取FileSystem的方式

      ``` java
      
      ```

4. hdfs小文件的合并 为什么进行小文件的合并.

   1. 通常在本地完成小文件的合并 , localfilesystem
   2. 在hdfs上完成小文件的合并
   3. har(归档)

## 2.Mapreduce介绍

### 分布式计算框架mapreduce的入门

### 1.mapreduce的核心思想

1. mapreduce: 分布式计算框架 , 用于离线计算.
2. 核心思想 : 分而治之 . 
   1. map阶段 : 主要负责任务的**分**  , 将一个大任务拆分成多个小任务.每个小任务之间不能产生影响,都是相对独立的.
   2. reduce阶段 : 主要负责任务的**合** , 将map的任务结果 进行聚合 .

### 2.设计构思

1. 对于数据的处理  :  分而治之
2. 构建抽象模型
   1. map 阶段
   2. reduce 阶段
   3. 这两个阶段 都是按照键 与 值的形式 进行数据的处理
3. 隐藏系统实现细节

### 3.整体结构

1. mapreduce

### 4.编程规范(**套路 : 天龙八部**)

1. 天龙八部 : mapreduce的开发步骤 按照一下八个步骤编写
2. **map阶段**
   1. 设置inputformat
   2. map

1. **shuffle阶段**
   1. 分区
   2. 排序
   3. 
2. **reduce阶段**
   1. reduce
   2. outputformat

### 5.wordcount实例编写

### 6.实例的运行环境的(**怎样运行的*??*)

### 7.在集群上运行jar

``` java
集群运行模式
（1）将mapreduce程序提交给yarn集群，分发到很多的节点上并发执行
（2）处理的数据和输出结果应该位于hdfs文件系统
（3）提交集群的实现步骤：
将程序打成JAR包，然后在集群的任意一个节点上用hadoop命令启动 
yarn jar hadoop_hdfs_operate-1.0-SNAPSHOT.jar cn.itcast.hdfs.demo1.JobMain

```

